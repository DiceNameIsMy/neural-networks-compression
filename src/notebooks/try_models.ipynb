{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a9bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(levelname)s: %(message)s\",\n",
    "    force=True,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(str(Path.cwd().parent.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e7233f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loading cached vertebral from /home/nur/Projects/vut-ip1-nn-quantization/datasets_cache/vertebral_cache.pkl\n",
      "DEBUG: Train Epoch:  1 [  32/248] Loss: 1.0933\n",
      "DEBUG: Train Epoch:  1 [ 192/248] Loss: 0.8740\n",
      "DEBUG: Test set: Average loss: 0.9553, Accuracy: 36/62 (58.06%)\n",
      "DEBUG: Train Epoch:  2 [  32/248] Loss: 0.8578\n",
      "DEBUG: Train Epoch:  2 [ 192/248] Loss: 0.9169\n",
      "DEBUG: Test set: Average loss: 0.6077, Accuracy: 45/62 (72.58%)\n",
      "DEBUG: Train Epoch:  3 [  32/248] Loss: 0.9042\n",
      "DEBUG: Train Epoch:  3 [ 192/248] Loss: 0.8703\n",
      "DEBUG: Test set: Average loss: 0.6276, Accuracy: 40/62 (64.52%)\n",
      "DEBUG: Train Epoch:  4 [  32/248] Loss: 0.9083\n",
      "DEBUG: Train Epoch:  4 [ 192/248] Loss: 0.9114\n",
      "DEBUG: Test set: Average loss: 0.5405, Accuracy: 49/62 (79.03%)\n",
      "DEBUG: Train Epoch:  5 [  32/248] Loss: 0.8263\n",
      "DEBUG: Train Epoch:  5 [ 192/248] Loss: 0.8478\n",
      "DEBUG: Test set: Average loss: 0.5457, Accuracy: 46/62 (74.19%)\n",
      "DEBUG: Train Epoch:  6 [  32/248] Loss: 0.8802\n",
      "DEBUG: Train Epoch:  6 [ 192/248] Loss: 0.8124\n",
      "DEBUG: Test set: Average loss: 0.5926, Accuracy: 49/62 (79.03%)\n",
      "DEBUG: Train Epoch:  7 [  32/248] Loss: 0.8175\n",
      "DEBUG: Train Epoch:  7 [ 192/248] Loss: 0.7823\n",
      "DEBUG: Test set: Average loss: 0.5868, Accuracy: 42/62 (67.74%)\n",
      "DEBUG: Train Epoch:  8 [  32/248] Loss: 0.8175\n",
      "DEBUG: Train Epoch:  8 [ 192/248] Loss: 0.8463\n",
      "DEBUG: Test set: Average loss: 0.4996, Accuracy: 50/62 (80.65%)\n",
      "DEBUG: Train Epoch:  9 [  32/248] Loss: 0.8513\n",
      "DEBUG: Train Epoch:  9 [ 192/248] Loss: 0.7911\n",
      "DEBUG: Test set: Average loss: 0.6005, Accuracy: 48/62 (77.42%)\n",
      "DEBUG: Train Epoch: 10 [  32/248] Loss: 0.8337\n",
      "DEBUG: Train Epoch: 10 [ 192/248] Loss: 0.7999\n",
      "DEBUG: Test set: Average loss: 0.6235, Accuracy: 48/62 (77.42%)\n",
      "DEBUG: Train Epoch: 11 [  32/248] Loss: 0.8713\n",
      "DEBUG: Train Epoch: 11 [ 192/248] Loss: 0.9026\n",
      "DEBUG: Test set: Average loss: 0.6743, Accuracy: 47/62 (75.81%)\n",
      "DEBUG: Train Epoch: 12 [  32/248] Loss: 0.7611\n",
      "DEBUG: Train Epoch: 12 [ 192/248] Loss: 0.8549\n",
      "DEBUG: Test set: Average loss: 0.6182, Accuracy: 48/62 (77.42%)\n",
      "DEBUG: Train Epoch: 13 [  32/248] Loss: 0.8249\n",
      "DEBUG: Train Epoch: 13 [ 192/248] Loss: 0.8410\n",
      "DEBUG: Test set: Average loss: 0.5859, Accuracy: 47/62 (75.81%)\n",
      "DEBUG: Train Epoch: 14 [  32/248] Loss: 0.7721\n",
      "DEBUG: Train Epoch: 14 [ 192/248] Loss: 0.7860\n",
      "DEBUG: Test set: Average loss: 0.6068, Accuracy: 45/62 (72.58%)\n",
      "DEBUG: Train Epoch: 15 [  32/248] Loss: 0.8212\n",
      "DEBUG: Train Epoch: 15 [ 192/248] Loss: 0.8359\n",
      "DEBUG: Test set: Average loss: 0.4809, Accuracy: 49/62 (79.03%)\n",
      "DEBUG: Test set: Average loss: 0.4809, Accuracy: 49/62 (79.03%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max': 79.03225806451613,\n",
       " 'mean': np.float64(79.03225806451613),\n",
       " 'std': np.float64(0.0)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.datasets.vertebral_dataset import VertebralDataset\n",
    "\n",
    "from src.models.mlp import (\n",
    "    MLPEvaluator,\n",
    "    MLPParams,\n",
    "    FCParams,\n",
    "    FCLayerParams,\n",
    "    WeightQuantMode,\n",
    ")\n",
    "from src.models.nn import ActivationModule, ActivationParams, NNTrainParams\n",
    "from src.models.quant.enums import QMode\n",
    "\n",
    "DatasetClass = VertebralDataset\n",
    "train_loader, test_loader = DatasetClass.get_dataloaders(batch_size=32)\n",
    "\n",
    "train_params = NNTrainParams(\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=15,\n",
    "    learning_rate=0.01,\n",
    "    weight_decay=0.0001,\n",
    "    early_stop_patience=10,\n",
    ")\n",
    "fc_params = FCParams(\n",
    "    layers=[\n",
    "        FCLayerParams(DatasetClass.input_size, WeightQuantMode.NBITS, 16),\n",
    "        FCLayerParams(32, WeightQuantMode.BINARY),\n",
    "        FCLayerParams(32, WeightQuantMode.BINARY),\n",
    "        FCLayerParams(DatasetClass.output_size, WeightQuantMode.BINARY),\n",
    "    ],\n",
    "    activation=ActivationParams(ActivationModule.BINARIZE_RESTE),\n",
    "    qmode=QMode.DET,\n",
    "    dropout_rate=0.0,\n",
    ")\n",
    "mlp_params = MLPParams(fc=fc_params, train=train_params)\n",
    "\n",
    "evaluator = MLPEvaluator(mlp_params)\n",
    "evaluator.evaluate_model(times=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "202de895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.constants import DEVICE\n",
    "from src.datasets.mnist_dataset import MiniMNISTDataset\n",
    "from src.models.cnn import CNN, CNNParams, ConvLayerParams, ConvParams\n",
    "\n",
    "CNNDatasetClass = MiniMNISTDataset\n",
    "cnn_train_loader, cnn_test_loader = CNNDatasetClass.get_dataloaders()\n",
    "\n",
    "conv_params = ConvParams(\n",
    "    in_channels=CNNDatasetClass.input_channels,\n",
    "    in_dimensions=CNNDatasetClass.input_dimensions,\n",
    "    in_bitwidth=8,\n",
    "    out_height=CNNDatasetClass.output_size,\n",
    "    layers=[\n",
    "        ConvLayerParams(channels=16, kernel_size=3, stride=1, padding=1),\n",
    "        ConvLayerParams(channels=32, kernel_size=3, stride=1, padding=1, pooling_kernel_size=2),\n",
    "    ],\n",
    "    activation=ActivationModule.BINARIZE,\n",
    "    qmode=QMode.DET,\n",
    "    dropout_rate=0.1,\n",
    ")\n",
    "cnn_fc_params = FCParams(\n",
    "    layers=[\n",
    "        FCLayerParams(-1, WeightQuantMode.NBITS, 16),\n",
    "        FCLayerParams(32, WeightQuantMode.BINARY),\n",
    "        FCLayerParams(32, WeightQuantMode.BINARY),\n",
    "        FCLayerParams(CNNDatasetClass.output_size, WeightQuantMode.BINARY),\n",
    "    ],\n",
    "    activation=ActivationParams(ActivationModule.BINARIZE_RESTE),\n",
    "    qmode=QMode.DET,\n",
    "    dropout_rate=0.0,\n",
    ")\n",
    "cnn_train_params = NNTrainParams(\n",
    "    cnn_train_loader,\n",
    "    cnn_test_loader,\n",
    "    epochs=1,\n",
    "    learning_rate=0.01,\n",
    "    weight_decay=0.0001,\n",
    "    early_stop_patience=10,\n",
    ")\n",
    "cnn_params = CNNParams(\n",
    "    in_bitwidth=8,\n",
    "    conv=conv_params,\n",
    "    fc=cnn_fc_params,\n",
    "    train=cnn_train_params,\n",
    ")\n",
    "cnn = CNN(cnn_params).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e90b08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Inspecting convolutional layers...\n",
      "INFO: Next layer shape: torch.Size([1, 16, 26, 26]), equating to 10816 inputs\n",
      "INFO: Next layer shape: torch.Size([1, 32, 12, 12]), equating to 4608 inputs\n",
      "INFO: FC input size is 4608\n"
     ]
    }
   ],
   "source": [
    "cnn.inspect_conv_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec108810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Train Epoch:  1 [ 128/4000] Loss: 2.4356\n",
      "DEBUG: Train Epoch:  1 [ 768/4000] Loss: 1.5990\n",
      "DEBUG: Train Epoch:  1 [1408/4000] Loss: 0.9766\n",
      "DEBUG: Train Epoch:  1 [2048/4000] Loss: 0.8390\n",
      "DEBUG: Train Epoch:  1 [2688/4000] Loss: 0.7298\n",
      "DEBUG: Train Epoch:  1 [3328/4000] Loss: 0.4537\n",
      "DEBUG: Train Epoch:  1 [3968/4000] Loss: 0.5184\n",
      "DEBUG: Test set: Average loss: 0.5844, Accuracy: 654/800 (81.75%)\n",
      "DEBUG: Test set: Average loss: 0.5844, Accuracy: 654/800 (81.75%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max': 81.75, 'mean': np.float64(81.75), 'std': np.float64(0.0)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.cnn import CNNEvaluator\n",
    "\n",
    "\n",
    "cnn_evaluator = CNNEvaluator(cnn_params)\n",
    "cnn_evaluator.evaluate_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39cc38e",
   "metadata": {},
   "source": [
    "# Quantization Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efae070f",
   "metadata": {},
   "source": [
    "## Techniques\n",
    "\n",
    "- Bitwidth quantization\n",
    "- BNN: Activation func (Using STE for gradients)\n",
    "- BNN_ReSTE: Activation func (Changing gradients: ReSTE)\n",
    "- TNN: Activation func (Differentiable? I'll need to implement it)\n",
    "- ... Find more?\n",
    "\n",
    "## How to integrate them\n",
    "\n",
    "Bitwidth quantization needs:\n",
    "- Quantization mode (Deterministic, Stochastic)\n",
    "- Per-layer: quantization level\n",
    "- Advances: Per perceptron quantization level. At least for the first layer\n",
    "\n",
    "Input bitwidth quantization. Closely tied to BNNs, as every other layer works with binary inputs\n",
    "- Per-input-neuron quantization level\n",
    "\n",
    "### BNN\n",
    "The idea is that every hidden layer only computes using binary (0/1, -1/+1) inputs & weights. Afaik bias isn't present. FC layer is simplified using popcount + ... operation. CNN in a similar mannet.\n",
    "- In the paper, they use hardtanh, ReLU. They also state that binarization itself is a form of non-linearity (used for hidden units). They use hardtanh even though they state that binarization itsef if a form of non-linearity. That's weird.\n",
    "\n",
    "Parameters:\n",
    "- None, I suppose.\n",
    "\n",
    "### BNN ReSTE\n",
    "Same as BNN, except for that:\n",
    "1. ReSTE is used instead of STE. ReSTE specifies a function to better estimate the quantized activation gradient.\n",
    "2. Gradients smaller than -1 and bigger than 1 are set to 0\n",
    "\n",
    "Parameters\n",
    "- o: Used for backprop. Modifies the approximated gradient.\n",
    "- t: Threshold\n",
    "- ...\n",
    "\n",
    "### TWN, Ternary weight networks\n",
    "Weights are one of: (-1,0,1). Paper focuses in CNNs. Nothing in mentioned of activation quantization. Typical pipeline is used: Conv -> BatchNorm -> Activation -> Maybe Pooling (every 2 conv layers?). Then FC layers.\n",
    "\n",
    "Notes:\n",
    "- The paper uses SGD.\n",
    "- We will likely combine it with neuron quantization.\n",
    "- FC & Conv do not have a bias.\n",
    "- I could apply something like ReSTE to this by using $y=\\frac{2arctan(10x^3)} {\\pi}$\n",
    "\n",
    "Parameters:\n",
    "- Threshold within which weight is set to 0?\n",
    "- ...\n",
    "\n",
    "### Idea: adaptive quantization -> static quantization\n",
    "During training use adaptive quantization, and for inference convert it to static quantization.\n",
    "- A potential problem: overflow, underflow?\n",
    "- ...\n",
    "\n",
    "### Some recap\n",
    "\n",
    "TWN and BNN do the same thing at its core: quantize weights. Extra things can be added like:\n",
    "- binary or ternary activation (activation returns either (-1/1) or (-1/0/1)), or ReLU + bitwidth quantization.\n",
    "- Input layer bitwidth quantization. (I should prefer per input quantization)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
