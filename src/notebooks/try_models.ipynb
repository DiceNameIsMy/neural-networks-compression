{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3a9bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(levelname)s: %(message)s\",\n",
    "    force=True,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(str(Path.cwd().parent.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e7233f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Caching breast_cancer to /home/nur/Projects/vut-ip1-nn-quantization/datasets_cache/breast_cancer_cache.pkl\n"
     ]
    }
   ],
   "source": [
    "from src.datasets.breast_cancer_dataset import BreastCancerDataset\n",
    "\n",
    "from src.models.mlp import (\n",
    "    MLPEvaluator,\n",
    "    KFoldMLPEvaluator,\n",
    "    MLPParams,\n",
    "    FCParams,\n",
    "    FCLayerParams,\n",
    "    WeightQuantMode,\n",
    ")\n",
    "from src.models.nn import ActivationModule, ActivationParams, NNTrainParams\n",
    "from src.models.quant.enums import QMode\n",
    "\n",
    "DatasetClass = BreastCancerDataset\n",
    "train_loader, test_loader = DatasetClass.get_dataloaders(batch_size=32)\n",
    "\n",
    "train_params = NNTrainParams(\n",
    "    DatasetClass,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=15,\n",
    "    learning_rate=0.01,\n",
    "    weight_decay=0.0001,\n",
    "    early_stop_patience=10,\n",
    ")\n",
    "fc_params = FCParams(\n",
    "    layers=[\n",
    "        FCLayerParams(DatasetClass.input_size, WeightQuantMode.NBITS, 32),\n",
    "        FCLayerParams(32, WeightQuantMode.BINARY),\n",
    "        FCLayerParams(32, WeightQuantMode.BINARY),\n",
    "        FCLayerParams(DatasetClass.output_size, WeightQuantMode.BINARY),\n",
    "    ],\n",
    "    activation=ActivationParams(ActivationModule.BINARIZE),\n",
    "    qmode=QMode.DET,\n",
    "    dropout_rate=0.0,\n",
    ")\n",
    "mlp_params = MLPParams(fc=fc_params, train=train_params)\n",
    "\n",
    "evaluator = MLPEvaluator(mlp_params)\n",
    "# evaluator.evaluate_model(times=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4700d57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.20e+02, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "         0.00e+00, 7.30e+01, 5.00e-01, 4.30e+01, 2.40e+00, 6.40e+01,\n",
       "         6.20e+01, 1.26e+02, 2.00e+00, 0.00e+00, 1.20e+02, 1.37e+02,\n",
       "         1.21e+02, 7.30e+01, 1.00e+00],\n",
       "        [1.32e+02, 6.00e-03, 0.00e+00, 6.00e-03, 3.00e-03, 0.00e+00,\n",
       "         0.00e+00, 1.70e+01, 2.10e+00, 0.00e+00, 1.04e+01, 1.30e+02,\n",
       "         6.80e+01, 1.98e+02, 6.00e+00, 1.00e+00, 1.41e+02, 1.36e+02,\n",
       "         1.40e+02, 1.20e+01, 0.00e+00],\n",
       "        [1.33e+02, 3.00e-03, 0.00e+00, 8.00e-03, 3.00e-03, 0.00e+00,\n",
       "         0.00e+00, 1.60e+01, 2.10e+00, 0.00e+00, 1.34e+01, 1.30e+02,\n",
       "         6.80e+01, 1.98e+02, 5.00e+00, 1.00e+00, 1.41e+02, 1.35e+02,\n",
       "         1.38e+02, 1.30e+01, 0.00e+00],\n",
       "        [1.34e+02, 3.00e-03, 0.00e+00, 8.00e-03, 3.00e-03, 0.00e+00,\n",
       "         0.00e+00, 1.60e+01, 2.40e+00, 0.00e+00, 2.30e+01, 1.17e+02,\n",
       "         5.30e+01, 1.70e+02, 1.10e+01, 0.00e+00, 1.37e+02, 1.34e+02,\n",
       "         1.37e+02, 1.30e+01, 1.00e+00],\n",
       "        [1.32e+02, 7.00e-03, 0.00e+00, 8.00e-03, 0.00e+00, 0.00e+00,\n",
       "         0.00e+00, 1.60e+01, 2.40e+00, 0.00e+00, 1.99e+01, 1.17e+02,\n",
       "         5.30e+01, 1.70e+02, 9.00e+00, 0.00e+00, 1.37e+02, 1.36e+02,\n",
       "         1.38e+02, 1.10e+01, 1.00e+00],\n",
       "        [1.34e+02, 1.00e-03, 0.00e+00, 1.00e-02, 9.00e-03, 0.00e+00,\n",
       "         2.00e-03, 2.60e+01, 5.90e+00, 0.00e+00, 0.00e+00, 1.50e+02,\n",
       "         5.00e+01, 2.00e+02, 5.00e+00, 3.00e+00, 7.60e+01, 1.07e+02,\n",
       "         1.07e+02, 1.70e+02, 0.00e+00],\n",
       "        [1.34e+02, 1.00e-03, 0.00e+00, 1.30e-02, 8.00e-03, 0.00e+00,\n",
       "         3.00e-03, 2.90e+01, 6.30e+00, 0.00e+00, 0.00e+00, 1.50e+02,\n",
       "         5.00e+01, 2.00e+02, 6.00e+00, 3.00e+00, 7.10e+01, 1.07e+02,\n",
       "         1.06e+02, 2.15e+02, 0.00e+00],\n",
       "        [1.22e+02, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "         0.00e+00, 8.30e+01, 5.00e-01, 6.00e+00, 1.56e+01, 6.80e+01,\n",
       "         6.20e+01, 1.30e+02, 0.00e+00, 0.00e+00, 1.22e+02, 1.22e+02,\n",
       "         1.23e+02, 3.00e+00, 1.00e+00],\n",
       "        [1.22e+02, 0.00e+00, 0.00e+00, 2.00e-03, 0.00e+00, 0.00e+00,\n",
       "         0.00e+00, 8.40e+01, 5.00e-01, 5.00e+00, 1.36e+01, 6.80e+01,\n",
       "         6.20e+01, 1.30e+02, 0.00e+00, 0.00e+00, 1.22e+02, 1.22e+02,\n",
       "         1.23e+02, 3.00e+00, 1.00e+00],\n",
       "        [1.22e+02, 0.00e+00, 0.00e+00, 3.00e-03, 0.00e+00, 0.00e+00,\n",
       "         0.00e+00, 8.60e+01, 3.00e-01, 6.00e+00, 1.06e+01, 6.80e+01,\n",
       "         6.20e+01, 1.30e+02, 1.00e+00, 0.00e+00, 1.22e+02, 1.22e+02,\n",
       "         1.23e+02, 1.00e+00, 1.00e+00]], dtype=float32),\n",
       " array([8, 5, 5, 5, 1, 7, 7, 8, 8, 8], dtype=int8))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = CardioDataset.get_xy()\n",
    "X[:10], y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6870bf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4600e+02, 3.0000e-03, 0.0000e+00, 3.0000e-03, 3.0000e-03, 0.0000e+00,\n",
      "        0.0000e+00, 3.7000e+01, 1.3000e+00, 3.1000e+01, 7.2000e+00, 7.9000e+01,\n",
      "        9.3000e+01, 1.7200e+02, 2.0000e+00, 2.0000e+00, 1.6200e+02, 1.5200e+02,\n",
      "        1.5600e+02, 2.3000e+01, 1.0000e+00]) tensor(5)\n"
     ]
    }
   ],
   "source": [
    "for x in train_loader:\n",
    "    print(x[0][0], x[1][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9044855d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Train Epoch:  1 [  32/221] Loss: 0.7075\n",
      "DEBUG: Train Epoch:  1 [ 192/221] Loss: 0.6934\n",
      "DEBUG: Test set: Average loss: 0.7651, Accuracy: 37/56 (66.07%)\n",
      "DEBUG: Train Epoch:  2 [  32/221] Loss: 0.5506\n",
      "DEBUG: Train Epoch:  2 [ 192/221] Loss: 0.5506\n",
      "DEBUG: Test set: Average loss: 0.7486, Accuracy: 40/56 (71.43%)\n",
      "DEBUG: Train Epoch:  3 [  32/221] Loss: 0.6458\n",
      "DEBUG: Train Epoch:  3 [ 192/221] Loss: 0.6881\n",
      "DEBUG: Test set: Average loss: 0.7918, Accuracy: 40/56 (71.43%)\n",
      "DEBUG: Train Epoch:  4 [  32/221] Loss: 0.6176\n",
      "DEBUG: Train Epoch:  4 [ 192/221] Loss: 0.6643\n",
      "DEBUG: Test set: Average loss: 0.8007, Accuracy: 40/56 (71.43%)\n",
      "DEBUG: Train Epoch:  5 [  32/221] Loss: 0.6369\n",
      "DEBUG: Train Epoch:  5 [ 192/221] Loss: 0.6071\n",
      "DEBUG: Test set: Average loss: 0.7384, Accuracy: 41/56 (73.21%)\n",
      "DEBUG: Train Epoch:  6 [  32/221] Loss: 0.5974\n",
      "DEBUG: Train Epoch:  6 [ 192/221] Loss: 0.5453\n",
      "DEBUG: Test set: Average loss: 0.7800, Accuracy: 40/56 (71.43%)\n",
      "DEBUG: Train Epoch:  7 [  32/221] Loss: 0.5736\n",
      "DEBUG: Train Epoch:  7 [ 192/221] Loss: 0.5595\n",
      "DEBUG: Test set: Average loss: 0.7623, Accuracy: 40/56 (71.43%)\n",
      "DEBUG: Train Epoch:  8 [  32/221] Loss: 0.5833\n",
      "DEBUG: Train Epoch:  8 [ 192/221] Loss: 0.5119\n",
      "DEBUG: Test set: Average loss: 0.7623, Accuracy: 40/56 (71.43%)\n",
      "DEBUG: Train Epoch:  9 [  32/221] Loss: 0.5312\n",
      "DEBUG: Train Epoch:  9 [ 192/221] Loss: 0.5498\n",
      "DEBUG: Test set: Average loss: 0.8024, Accuracy: 39/56 (69.64%)\n",
      "DEBUG: Train Epoch: 10 [  32/221] Loss: 0.6450\n",
      "DEBUG: Train Epoch: 10 [ 192/221] Loss: 0.5357\n",
      "DEBUG: Test set: Average loss: 0.7712, Accuracy: 40/56 (71.43%)\n",
      "DEBUG: Train Epoch: 11 [  32/221] Loss: 0.6264\n",
      "DEBUG: Train Epoch: 11 [ 192/221] Loss: 0.5929\n",
      "DEBUG: Test set: Average loss: 0.7712, Accuracy: 40/56 (71.43%)\n",
      "DEBUG: Train Epoch: 12 [  32/221] Loss: 0.5595\n",
      "DEBUG: Train Epoch: 12 [ 192/221] Loss: 0.6926\n",
      "DEBUG: Test set: Average loss: 0.7623, Accuracy: 40/56 (71.43%)\n",
      "DEBUG: Train Epoch: 13 [  32/221] Loss: 0.5119\n",
      "DEBUG: Train Epoch: 13 [ 192/221] Loss: 0.6405\n",
      "DEBUG: Test set: Average loss: 0.7623, Accuracy: 40/56 (71.43%)\n",
      "DEBUG: Train Epoch: 14 [  32/221] Loss: 0.5409\n",
      "DEBUG: Train Epoch: 14 [ 192/221] Loss: 0.5974\n",
      "DEBUG: Test set: Average loss: 0.7207, Accuracy: 41/56 (73.21%)\n",
      "DEBUG: Train Epoch: 15 [  32/221] Loss: 0.5260\n",
      "DEBUG: Train Epoch: 15 [ 192/221] Loss: 0.5974\n",
      "DEBUG: Test set: Average loss: 0.7413, Accuracy: 40/56 (71.43%)\n",
      "DEBUG: Test set: Average loss: 0.7413, Accuracy: 40/56 (71.43%)\n",
      "DEBUG: Train Epoch:  1 [  32/221] Loss: 0.6325\n",
      "DEBUG: Train Epoch:  1 [ 192/221] Loss: 0.6422\n",
      "DEBUG: Test set: Average loss: 0.6610, Accuracy: 40/56 (71.43%)\n",
      "DEBUG: Train Epoch:  2 [  32/221] Loss: 0.5321\n",
      "DEBUG: Train Epoch:  2 [ 192/221] Loss: 0.7463\n",
      "DEBUG: Test set: Average loss: 0.9206, Accuracy: 36/56 (64.29%)\n",
      "DEBUG: Train Epoch:  3 [  32/221] Loss: 0.6555\n",
      "DEBUG: Train Epoch:  3 [ 192/221] Loss: 0.5409\n",
      "DEBUG: Test set: Average loss: 0.8658, Accuracy: 36/56 (64.29%)\n",
      "DEBUG: Train Epoch:  4 [  32/221] Loss: 0.5797\n",
      "DEBUG: Train Epoch:  4 [ 192/221] Loss: 0.5833\n",
      "DEBUG: Test set: Average loss: 0.8437, Accuracy: 37/56 (66.07%)\n",
      "DEBUG: Train Epoch:  5 [  32/221] Loss: 0.5833\n",
      "DEBUG: Train Epoch:  5 [ 192/221] Loss: 0.6405\n",
      "DEBUG: Test set: Average loss: 0.7918, Accuracy: 38/56 (67.86%)\n",
      "DEBUG: Train Epoch:  6 [  32/221] Loss: 0.6071\n",
      "DEBUG: Train Epoch:  6 [ 192/221] Loss: 0.5357\n",
      "DEBUG: Test set: Average loss: 0.8541, Accuracy: 36/56 (64.29%)\n",
      "DEBUG: Train Epoch:  7 [  32/221] Loss: 0.6071\n",
      "DEBUG: Train Epoch:  7 [ 192/221] Loss: 0.5885\n",
      "DEBUG: Test set: Average loss: 0.8854, Accuracy: 36/56 (64.29%)\n",
      "DEBUG: Train Epoch:  8 [  32/221] Loss: 0.5833\n",
      "DEBUG: Train Epoch:  8 [ 192/221] Loss: 0.6405\n",
      "DEBUG: Test set: Average loss: 0.9168, Accuracy: 34/56 (60.71%)\n",
      "DEBUG: Train Epoch:  9 [  32/221] Loss: 0.5736\n",
      "DEBUG: Train Epoch:  9 [ 192/221] Loss: 0.6450\n",
      "DEBUG: Test set: Average loss: 0.8561, Accuracy: 36/56 (64.29%)\n",
      "DEBUG: Train Epoch: 10 [  32/221] Loss: 0.5974\n",
      "DEBUG: Train Epoch: 10 [ 192/221] Loss: 0.5550\n",
      "DEBUG: Test set: Average loss: 1.1283, Accuracy: 27/56 (48.21%)\n",
      "DEBUG: Train Epoch: 11 [  32/221] Loss: 0.7216\n",
      "DEBUG: Train Epoch: 11 [ 192/221] Loss: 0.4977\n",
      "DEBUG: Test set: Average loss: 0.9778, Accuracy: 32/56 (57.14%)\n",
      "DEBUG: Train Epoch: 12 [  32/221] Loss: 0.6071\n",
      "DEBUG: Train Epoch: 12 [ 192/221] Loss: 0.6026\n",
      "DEBUG: Test set: Average loss: 0.9601, Accuracy: 33/56 (58.93%)\n",
      "DEBUG: Train Epoch: 13 [  32/221] Loss: 0.5119\n",
      "DEBUG: Train Epoch: 13 [ 192/221] Loss: 0.5260\n",
      "DEBUG: Test set: Average loss: 0.8960, Accuracy: 35/56 (62.50%)\n",
      "DEBUG: Train Epoch: 14 [  32/221] Loss: 0.5022\n",
      "DEBUG: Train Epoch: 14 [ 192/221] Loss: 0.5736\n",
      "DEBUG: Test set: Average loss: 1.0137, Accuracy: 30/56 (53.57%)\n",
      "DEBUG: Train Epoch: 15 [  32/221] Loss: 0.5736\n",
      "DEBUG: Train Epoch: 15 [ 192/221] Loss: 0.5498\n",
      "DEBUG: Test set: Average loss: 0.9106, Accuracy: 35/56 (62.50%)\n",
      "DEBUG: Test set: Average loss: 0.9106, Accuracy: 35/56 (62.50%)\n",
      "DEBUG: Train Epoch:  1 [  32/222] Loss: 0.7330\n",
      "DEBUG: Train Epoch:  1 [ 192/222] Loss: 0.6378\n",
      "DEBUG: Test set: Average loss: 0.7500, Accuracy: 38/55 (69.09%)\n",
      "DEBUG: Train Epoch:  2 [  32/222] Loss: 0.6264\n",
      "DEBUG: Train Epoch:  2 [ 192/222] Loss: 0.6281\n",
      "DEBUG: Test set: Average loss: 0.6616, Accuracy: 42/55 (76.36%)\n",
      "DEBUG: Train Epoch:  3 [  32/222] Loss: 0.5603\n",
      "DEBUG: Train Epoch:  3 [ 192/222] Loss: 0.6890\n",
      "DEBUG: Test set: Average loss: 0.5925, Accuracy: 45/55 (81.82%)\n",
      "DEBUG: Train Epoch:  4 [  32/222] Loss: 0.5982\n",
      "DEBUG: Train Epoch:  4 [ 192/222] Loss: 0.6934\n",
      "DEBUG: Test set: Average loss: 0.5389, Accuracy: 46/55 (83.64%)\n",
      "DEBUG: Train Epoch:  5 [  32/222] Loss: 0.6599\n",
      "DEBUG: Train Epoch:  5 [ 192/222] Loss: 0.6555\n",
      "DEBUG: Test set: Average loss: 0.5337, Accuracy: 43/55 (78.18%)\n",
      "DEBUG: Train Epoch:  6 [  32/222] Loss: 0.6087\n",
      "DEBUG: Train Epoch:  6 [ 192/222] Loss: 0.6220\n",
      "DEBUG: Test set: Average loss: 0.5592, Accuracy: 45/55 (81.82%)\n",
      "DEBUG: Train Epoch:  7 [  32/222] Loss: 0.5990\n",
      "DEBUG: Train Epoch:  7 [ 192/222] Loss: 0.6035\n",
      "DEBUG: Test set: Average loss: 0.5805, Accuracy: 43/55 (78.18%)\n",
      "DEBUG: Train Epoch:  8 [  32/222] Loss: 0.6599\n",
      "DEBUG: Train Epoch:  8 [ 192/222] Loss: 0.7075\n",
      "DEBUG: Test set: Average loss: 0.5416, Accuracy: 43/55 (78.18%)\n",
      "DEBUG: Train Epoch:  9 [  32/222] Loss: 0.6317\n",
      "DEBUG: Train Epoch:  9 [ 192/222] Loss: 0.6502\n",
      "DEBUG: Test set: Average loss: 0.6774, Accuracy: 42/55 (76.36%)\n",
      "DEBUG: Train Epoch: 10 [  32/222] Loss: 0.7357\n",
      "DEBUG: Train Epoch: 10 [ 192/222] Loss: 0.6212\n",
      "DEBUG: Test set: Average loss: 0.5116, Accuracy: 45/55 (81.82%)\n",
      "DEBUG: Train Epoch: 11 [  32/222] Loss: 0.6123\n",
      "DEBUG: Train Epoch: 11 [ 192/222] Loss: 0.6309\n",
      "DEBUG: Test set: Average loss: 0.5550, Accuracy: 44/55 (80.00%)\n",
      "DEBUG: Train Epoch: 12 [  32/222] Loss: 0.5550\n",
      "DEBUG: Train Epoch: 12 [ 192/222] Loss: 0.6264\n",
      "DEBUG: Test set: Average loss: 0.5204, Accuracy: 46/55 (83.64%)\n",
      "DEBUG: Train Epoch: 13 [  32/222] Loss: 0.5498\n",
      "DEBUG: Train Epoch: 13 [ 192/222] Loss: 0.6123\n",
      "DEBUG: Test set: Average loss: 0.4715, Accuracy: 45/55 (81.82%)\n",
      "DEBUG: Train Epoch: 14 [  32/222] Loss: 0.6599\n",
      "DEBUG: Train Epoch: 14 [ 192/222] Loss: 0.5462\n",
      "DEBUG: Test set: Average loss: 0.5015, Accuracy: 45/55 (81.82%)\n",
      "DEBUG: Train Epoch: 15 [  32/222] Loss: 0.6696\n",
      "DEBUG: Train Epoch: 15 [ 192/222] Loss: 0.5833\n",
      "DEBUG: Test set: Average loss: 0.4504, Accuracy: 47/55 (85.45%)\n",
      "DEBUG: Test set: Average loss: 0.4504, Accuracy: 47/55 (85.45%)\n",
      "DEBUG: Train Epoch:  1 [  32/222] Loss: 0.7233\n",
      "DEBUG: Train Epoch:  1 [ 192/222] Loss: 0.6749\n",
      "DEBUG: Test set: Average loss: 0.7891, Accuracy: 37/55 (67.27%)\n",
      "DEBUG: Train Epoch:  2 [  32/222] Loss: 0.6845\n",
      "DEBUG: Train Epoch:  2 [ 192/222] Loss: 0.5603\n",
      "DEBUG: Test set: Average loss: 0.7734, Accuracy: 39/55 (70.91%)\n",
      "DEBUG: Train Epoch:  3 [  32/222] Loss: 0.6369\n",
      "DEBUG: Train Epoch:  3 [ 192/222] Loss: 0.5788\n",
      "DEBUG: Test set: Average loss: 0.8081, Accuracy: 36/55 (65.45%)\n",
      "DEBUG: Train Epoch:  4 [  32/222] Loss: 0.5603\n",
      "DEBUG: Train Epoch:  4 [ 192/222] Loss: 0.5409\n",
      "DEBUG: Test set: Average loss: 0.7605, Accuracy: 36/55 (65.45%)\n",
      "DEBUG: Train Epoch:  5 [  32/222] Loss: 0.5982\n",
      "DEBUG: Train Epoch:  5 [ 192/222] Loss: 0.6167\n",
      "DEBUG: Test set: Average loss: 0.7888, Accuracy: 40/55 (72.73%)\n",
      "DEBUG: Train Epoch:  6 [  32/222] Loss: 0.5788\n",
      "DEBUG: Train Epoch:  6 [ 192/222] Loss: 0.5974\n",
      "DEBUG: Test set: Average loss: 0.7983, Accuracy: 37/55 (67.27%)\n",
      "DEBUG: Train Epoch:  7 [  32/222] Loss: 0.5788\n",
      "DEBUG: Train Epoch:  7 [ 192/222] Loss: 0.6123\n",
      "DEBUG: Test set: Average loss: 0.8541, Accuracy: 36/55 (65.45%)\n",
      "DEBUG: Train Epoch:  8 [  32/222] Loss: 0.6026\n",
      "DEBUG: Train Epoch:  8 [ 192/222] Loss: 0.5595\n",
      "DEBUG: Test set: Average loss: 0.7493, Accuracy: 39/55 (70.91%)\n",
      "DEBUG: Train Epoch:  9 [  32/222] Loss: 0.5788\n",
      "DEBUG: Train Epoch:  9 [ 192/222] Loss: 0.5312\n",
      "DEBUG: Test set: Average loss: 0.8383, Accuracy: 37/55 (67.27%)\n",
      "DEBUG: Train Epoch: 10 [  32/222] Loss: 0.5833\n",
      "DEBUG: Train Epoch: 10 [ 192/222] Loss: 0.6309\n",
      "DEBUG: Test set: Average loss: 0.9389, Accuracy: 33/55 (60.00%)\n",
      "DEBUG: Train Epoch: 11 [  32/222] Loss: 0.5691\n",
      "DEBUG: Train Epoch: 11 [ 192/222] Loss: 0.5119\n",
      "DEBUG: Test set: Average loss: 0.8831, Accuracy: 35/55 (63.64%)\n",
      "DEBUG: Train Epoch: 12 [  32/222] Loss: 0.5260\n",
      "DEBUG: Train Epoch: 12 [ 192/222] Loss: 0.6026\n",
      "DEBUG: Test set: Average loss: 0.8643, Accuracy: 36/55 (65.45%)\n",
      "DEBUG: Train Epoch: 13 [  32/222] Loss: 0.5974\n",
      "DEBUG: Train Epoch: 13 [ 192/222] Loss: 0.6547\n",
      "DEBUG: Test set: Average loss: 0.8696, Accuracy: 36/55 (65.45%)\n",
      "DEBUG: Train Epoch: 14 [  32/222] Loss: 0.5603\n",
      "DEBUG: Train Epoch: 14 [ 192/222] Loss: 0.6547\n",
      "DEBUG: Test set: Average loss: 0.8182, Accuracy: 38/55 (69.09%)\n",
      "DEBUG: Train Epoch: 15 [  32/222] Loss: 0.5929\n",
      "DEBUG: Train Epoch: 15 [ 192/222] Loss: 0.6167\n",
      "DEBUG: Test set: Average loss: 0.8977, Accuracy: 32/55 (58.18%)\n",
      "DEBUG: Test set: Average loss: 0.8977, Accuracy: 32/55 (58.18%)\n",
      "DEBUG: Train Epoch:  1 [  32/222] Loss: 0.5902\n",
      "DEBUG: Train Epoch:  1 [ 192/222] Loss: 0.7225\n",
      "DEBUG: Test set: Average loss: 0.6271, Accuracy: 44/55 (80.00%)\n",
      "DEBUG: Train Epoch:  2 [  32/222] Loss: 0.6035\n",
      "DEBUG: Train Epoch:  2 [ 192/222] Loss: 0.5417\n",
      "DEBUG: Test set: Average loss: 0.6363, Accuracy: 43/55 (78.18%)\n",
      "DEBUG: Train Epoch:  3 [  32/222] Loss: 0.5982\n",
      "DEBUG: Train Epoch:  3 [ 192/222] Loss: 0.6652\n",
      "DEBUG: Test set: Average loss: 0.5492, Accuracy: 45/55 (81.82%)\n",
      "DEBUG: Train Epoch:  4 [  32/222] Loss: 0.6317\n",
      "DEBUG: Train Epoch:  4 [ 192/222] Loss: 0.6450\n",
      "DEBUG: Test set: Average loss: 0.6429, Accuracy: 43/55 (78.18%)\n",
      "DEBUG: Train Epoch:  5 [  32/222] Loss: 0.5462\n",
      "DEBUG: Train Epoch:  5 [ 192/222] Loss: 0.6890\n",
      "DEBUG: Test set: Average loss: 0.6542, Accuracy: 42/55 (76.36%)\n",
      "DEBUG: Train Epoch:  6 [  32/222] Loss: 0.6502\n",
      "DEBUG: Train Epoch:  6 [ 192/222] Loss: 0.5885\n",
      "DEBUG: Test set: Average loss: 0.6128, Accuracy: 43/55 (78.18%)\n",
      "DEBUG: Train Epoch:  7 [  32/222] Loss: 0.7410\n",
      "DEBUG: Train Epoch:  7 [ 192/222] Loss: 0.5224\n",
      "DEBUG: Test set: Average loss: 0.5592, Accuracy: 45/55 (81.82%)\n",
      "DEBUG: Train Epoch:  8 [  32/222] Loss: 0.6599\n",
      "DEBUG: Train Epoch:  8 [ 192/222] Loss: 0.6458\n",
      "DEBUG: Test set: Average loss: 0.5930, Accuracy: 43/55 (78.18%)\n",
      "DEBUG: Train Epoch:  9 [  32/222] Loss: 0.6405\n",
      "DEBUG: Train Epoch:  9 [ 192/222] Loss: 0.5224\n",
      "DEBUG: Test set: Average loss: 0.6352, Accuracy: 42/55 (76.36%)\n",
      "DEBUG: Train Epoch: 10 [  32/222] Loss: 0.6599\n",
      "DEBUG: Train Epoch: 10 [ 192/222] Loss: 0.5744\n",
      "DEBUG: Test set: Average loss: 0.5762, Accuracy: 43/55 (78.18%)\n",
      "DEBUG: Train Epoch: 11 [  32/222] Loss: 0.6264\n",
      "DEBUG: Train Epoch: 11 [ 192/222] Loss: 0.5788\n",
      "DEBUG: Test set: Average loss: 0.5762, Accuracy: 44/55 (80.00%)\n",
      "DEBUG: Train Epoch: 12 [  32/222] Loss: 0.5559\n",
      "DEBUG: Train Epoch: 12 [ 192/222] Loss: 0.5647\n",
      "DEBUG: Test set: Average loss: 0.5449, Accuracy: 44/55 (80.00%)\n",
      "DEBUG: Train Epoch: 13 [  32/222] Loss: 0.6599\n",
      "DEBUG: Train Epoch: 13 [ 192/222] Loss: 0.6599\n",
      "DEBUG: Test set: Average loss: 0.5427, Accuracy: 45/55 (81.82%)\n",
      "DEBUG: Train Epoch: 14 [  32/222] Loss: 0.5885\n",
      "DEBUG: Train Epoch: 14 [ 192/222] Loss: 0.6026\n",
      "DEBUG: Test set: Average loss: 0.5605, Accuracy: 44/55 (80.00%)\n",
      "DEBUG: Train Epoch: 15 [  32/222] Loss: 0.5409\n",
      "DEBUG: Train Epoch: 15 [ 192/222] Loss: 0.5833\n",
      "DEBUG: Test set: Average loss: 0.5340, Accuracy: 43/55 (78.18%)\n",
      "DEBUG: Test set: Average loss: 0.5340, Accuracy: 43/55 (78.18%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max': 85.45454545454545,\n",
       " 'mean': np.float64(71.14935064935065),\n",
       " 'std': np.float64(9.971362266062368),\n",
       " 'accuracies': [71.42857142857143,\n",
       "  62.5,\n",
       "  85.45454545454545,\n",
       "  58.18181818181818,\n",
       "  78.18181818181819]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KFoldMLPEvaluator(mlp_params).evaluate_model(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "202de895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nur/Projects/vut-ip1-nn-quantization/src/datasets/dataset.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(X, dtype=torch.float32)\n",
      "/home/nur/Projects/vut-ip1-nn-quantization/src/datasets/dataset.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.y = torch.tensor(y, dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "from src.constants import DEVICE\n",
    "from src.datasets.mnist_dataset import MiniMNISTDataset\n",
    "from src.models.cnn import CNN, CNNParams, ConvLayerParams, ConvParams\n",
    "\n",
    "CNNDatasetClass = MiniMNISTDataset\n",
    "cnn_train_loader, cnn_test_loader = CNNDatasetClass.get_dataloaders()\n",
    "\n",
    "conv_params = ConvParams(\n",
    "    in_channels=CNNDatasetClass.input_channels,\n",
    "    in_dimensions=CNNDatasetClass.input_dimensions,\n",
    "    in_bitwidth=8,\n",
    "    out_height=CNNDatasetClass.output_size,\n",
    "    layers=[\n",
    "        ConvLayerParams(channels=16, kernel_size=3, stride=1, padding=1),\n",
    "        ConvLayerParams(channels=32, kernel_size=3, stride=1, padding=1, pooling_kernel_size=2),\n",
    "    ],\n",
    "    activation=ActivationModule.BINARIZE,\n",
    "    qmode=QMode.DET,\n",
    "    reste_o=3,\n",
    "    reste_threshold=1.5,\n",
    "    dropout_rate=0.1,\n",
    ")\n",
    "cnn_fc_params = FCParams(\n",
    "    layers=[\n",
    "        FCLayerParams(-1, WeightQuantMode.NBITS, 16),\n",
    "        FCLayerParams(32, WeightQuantMode.BINARY),\n",
    "        FCLayerParams(32, WeightQuantMode.BINARY),\n",
    "        FCLayerParams(CNNDatasetClass.output_size, WeightQuantMode.BINARY),\n",
    "    ],\n",
    "    activation=ActivationParams(ActivationModule.BINARIZE_RESTE),\n",
    "    qmode=QMode.DET,\n",
    "    dropout_rate=0.0,\n",
    ")\n",
    "cnn_train_params = NNTrainParams(\n",
    "    CNNDatasetClass,\n",
    "    cnn_train_loader,\n",
    "    cnn_test_loader,\n",
    "    epochs=1,\n",
    "    learning_rate=0.01,\n",
    "    weight_decay=0.0001,\n",
    "    early_stop_patience=10,\n",
    ")\n",
    "cnn_params = CNNParams(\n",
    "    in_bitwidth=8,\n",
    "    conv=conv_params,\n",
    "    fc=cnn_fc_params,\n",
    "    train=cnn_train_params,\n",
    ")\n",
    "cnn = CNN(cnn_params).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feda289c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 5, 6, 7, 6, 5, 3, 0, 7, 8, 7, 3, 1, 8, 1, 3, 2, 4, 9, 8, 2, 5, 8, 1,\n",
      "        0, 5, 1, 3, 7, 6, 3, 3, 6, 7, 8, 4, 5, 6, 6, 3, 9, 7, 6, 1, 3, 3, 3, 2,\n",
      "        5, 3, 1, 8, 4, 2, 9, 6, 6, 9, 0, 6, 9, 0, 3, 9, 6, 3, 5, 4, 5, 3, 7, 7,\n",
      "        2, 7, 5, 3, 2, 3, 9, 0, 6, 2, 5, 9, 0, 0, 4, 1, 1, 3, 3, 5, 3, 1, 3, 0,\n",
      "        1, 3, 7, 3, 0, 8, 6, 8, 9, 2, 8, 0, 7, 7, 3, 2, 3, 0, 5, 3, 3, 1, 1, 4,\n",
      "        4, 4, 7, 2, 1, 7, 8, 7])\n"
     ]
    }
   ],
   "source": [
    "for x in cnn_train_loader:\n",
    "    print(x[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28a9c71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1702,  0.1702, -0.1702],\n",
       "         [-0.1702, -0.1702, -0.1702],\n",
       "         [ 0.1702, -0.1702,  0.1702]]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.conv_layers[0][0].weight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b93b385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Train Epoch:  1 [ 128/3200] Loss: 2.2389\n",
      "DEBUG: Train Epoch:  1 [ 768/3200] Loss: 1.9715\n",
      "DEBUG: Train Epoch:  1 [1408/3200] Loss: 1.1675\n",
      "DEBUG: Train Epoch:  1 [2048/3200] Loss: 0.8088\n",
      "DEBUG: Train Epoch:  1 [2688/3200] Loss: 0.6602\n",
      "DEBUG: Test set: Average loss: 0.8538, Accuracy: 580/800 (72.50%)\n",
      "DEBUG: Test set: Average loss: 0.8538, Accuracy: 580/800 (72.50%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max': 72.5,\n",
       " 'mean': np.float64(72.5),\n",
       " 'std': np.float64(0.0),\n",
       " 'accuracies': [72.5]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.cnn import CNNEvaluator\n",
    "\n",
    "\n",
    "CNNEvaluator(cnn_params).evaluate_model(times=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e851a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.nn import save_model\n",
    "\n",
    "\n",
    "save_model(cnn, \"cnn_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ad340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.nn import load_model\n",
    "\n",
    "\n",
    "cnn2 = load_model(cnn, \"cnn_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7129b3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1843, -0.1843, -0.1843],\n",
       "         [ 0.1843,  0.1843,  0.1843],\n",
       "         [ 0.1843,  0.1843,  0.1843]]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn2.conv_layers[0][0].weight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e90b08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Inspecting convolutional layers...\n",
      "INFO: Next layer shape: torch.Size([1, 16, 28, 28]), equating to 12544 inputs\n",
      "INFO: Next layer shape: torch.Size([1, 32, 14, 14]), equating to 6272 inputs\n",
      "INFO: FC input size is 6272\n"
     ]
    }
   ],
   "source": [
    "cnn.inspect_conv_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec108810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Train Epoch:  1 [ 128/4000] Loss: 2.4665\n",
      "DEBUG: Train Epoch:  1 [ 768/4000] Loss: 1.4402\n",
      "DEBUG: Train Epoch:  1 [1408/4000] Loss: 1.1744\n",
      "DEBUG: Train Epoch:  1 [2048/4000] Loss: 1.0672\n",
      "DEBUG: Train Epoch:  1 [2688/4000] Loss: 0.8749\n",
      "DEBUG: Train Epoch:  1 [3328/4000] Loss: 0.5647\n",
      "DEBUG: Train Epoch:  1 [3968/4000] Loss: 0.4820\n",
      "DEBUG: Test set: Average loss: 0.8719, Accuracy: 579/800 (72.38%)\n",
      "DEBUG: Test set: Average loss: 0.8719, Accuracy: 579/800 (72.38%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max': 72.375, 'mean': np.float64(72.375), 'std': np.float64(0.0)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.cnn import CNNEvaluator\n",
    "\n",
    "\n",
    "cnn_evaluator = CNNEvaluator(cnn_params)\n",
    "cnn_evaluator.evaluate_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39cc38e",
   "metadata": {},
   "source": [
    "# Quantization Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efae070f",
   "metadata": {},
   "source": [
    "## Techniques\n",
    "\n",
    "- Bitwidth quantization\n",
    "- BNN: Activation func (Using STE for gradients)\n",
    "- BNN_ReSTE: Activation func (Changing gradients: ReSTE)\n",
    "- TNN: Activation func (Differentiable? I'll need to implement it)\n",
    "- ... Find more?\n",
    "\n",
    "## How to integrate them\n",
    "\n",
    "Bitwidth quantization needs:\n",
    "- Quantization mode (Deterministic, Stochastic)\n",
    "- Per-layer: quantization level\n",
    "- Advances: Per perceptron quantization level. At least for the first layer\n",
    "\n",
    "Input bitwidth quantization. Closely tied to BNNs, as every other layer works with binary inputs\n",
    "- Per-input-neuron quantization level\n",
    "\n",
    "### BNN\n",
    "The idea is that every hidden layer only computes using binary (0/1, -1/+1) inputs & weights. Afaik bias isn't present. FC layer is simplified using popcount + ... operation. CNN in a similar mannet.\n",
    "- In the paper, they use hardtanh, ReLU. They also state that binarization itself is a form of non-linearity (used for hidden units). They use hardtanh even though they state that binarization itsef if a form of non-linearity. That's weird.\n",
    "\n",
    "Parameters:\n",
    "- None, I suppose.\n",
    "\n",
    "### BNN ReSTE\n",
    "Same as BNN, except for that:\n",
    "1. ReSTE is used instead of STE. ReSTE specifies a function to better estimate the quantized activation gradient.\n",
    "2. Gradients smaller than -1 and bigger than 1 are set to 0\n",
    "\n",
    "Parameters\n",
    "- o: Used for backprop. Modifies the approximated gradient.\n",
    "- t: Threshold\n",
    "- ...\n",
    "\n",
    "### TWN, Ternary weight networks\n",
    "Weights are one of: (-1,0,1). Paper focuses in CNNs. Nothing in mentioned of activation quantization. Typical pipeline is used: Conv -> BatchNorm -> Activation -> Maybe Pooling (every 2 conv layers?). Then FC layers.\n",
    "\n",
    "Notes:\n",
    "- The paper uses SGD.\n",
    "- We will likely combine it with neuron quantization.\n",
    "- FC & Conv do not have a bias.\n",
    "- I could apply something like ReSTE to this by using $y=\\frac{2arctan(10x^3)} {\\pi}$\n",
    "\n",
    "Parameters:\n",
    "- Threshold within which weight is set to 0?\n",
    "- ...\n",
    "\n",
    "### Idea: adaptive quantization -> static quantization\n",
    "During training use adaptive quantization, and for inference convert it to static quantization.\n",
    "- A potential problem: overflow, underflow?\n",
    "- ...\n",
    "\n",
    "### Some recap\n",
    "\n",
    "TWN and BNN do the same thing at its core: quantize weights. Extra things can be added like:\n",
    "- binary or ternary activation (activation returns either (-1/1) or (-1/0/1)), or ReLU + bitwidth quantization.\n",
    "- Input layer bitwidth quantization. (I should prefer per input quantization)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
